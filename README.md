<h1 align="center"> <p>DyDiT</p></h1>


<p align="center">
  <picture>
    <img width="40%" alt="Dynamic Diffusion Transformer" src="./DyDiT/assets/logo.png">
  </picture>
</p>



The official implementation of "2025ICLR Dynamic Diffusion Transformer" and "2025ArXiv DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation".



## üöÄ News
- `2025.03.26` We release the code of training and text-to-image generation model, DyFLUX.
- `2025.01.23` "Dynamic Diffusion Transformer" is accepted by ICLR 2025!!! We will update the code and paper soon.
- `2024.12.19:` We release the code for inference. 
- `2024.10.04:` Our paper is released.






## ü§î Cite DyDiT
If you found our work useful, please consider citing us.
```
@article{zhao2024dynamic,
  title={Dynamic diffusion transformer},
  author={Zhao, Wangbo and Han, Yizeng and Tang, Jiasheng and Wang, Kai and Song, Yibing and Huang, Gao and Wang, Fan and You, Yang},
  journal={arXiv preprint arXiv:2410.03456},
  year={2024}
}

@article{zhao2025dynamic,
  title={DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation},
  author={Zhao, Wangbo and Han, Yizeng and Tang, Jiasheng and Wang, Kai and Luo, Hao and Song, Yibing and Huang, Gao and Wang, Fan and You, Yang},
  year={2025}
}

```

## ‚òéÔ∏è Contact
If you're interested in collaborating with us, feel free to reach out via email at wangbo.zhao96@gmail.com.
