<h1 align="center"> <p>DyDiT</p></h1>


<p align="center">
  <picture>
    <img width="40%" alt="Dynamic Diffusion Transformer" src="./DyDiT/assets/logo.png">
  </picture>
</p>



The official implementation of two papers:
  + "2025ICLR Dynamic Diffusion Transformer"
  + "2025ArXiv DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation"



## üöÄ News
- `2025.03.26` We release the code of training and text-to-image generation model, DyFLUX.
- `2025.01.23` "Dynamic Diffusion Transformer" is accepted by ICLR 2025!!! We will update the code and paper soon.
- `2024.12.19:` We release the code for inference. 
- `2024.10.04:` Our paper is released.



## üîß Usage
We provide detailed instructions to run our code. Please `cd DyDiT` or `cd DyFLUX` for more information.

## ü§î Cite DyDiT
If you found our work useful, please consider citing us.
```
@article{zhao2024dynamic,
  title={Dynamic diffusion transformer},
  author={Zhao, Wangbo and Han, Yizeng and Tang, Jiasheng and Wang, Kai and Song, Yibing and Huang, Gao and Wang, Fan and You, Yang},
  journal={ICLR},
  year={2025}
}

@article{zhao2025dynamic,
  title={DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation},
  author={Zhao, Wangbo and Han, Yizeng and Tang, Jiasheng and Wang, Kai and Luo, Hao and Song, Yibing and Huang, Gao and Wang, Fan and You, Yang},
  year={2025}
}

```

## ‚òéÔ∏è Contact
If you're interested in collaborating with us, feel free to reach out via email at wangbo.zhao96@gmail.com.
